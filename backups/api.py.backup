import os
import logging
import requests
import json
import re
from flask import Blueprint, request, jsonify
from bs4 import BeautifulSoup
import urllib.parse

# Configure logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Create blueprint
api_bp = Blueprint('api', __name__, url_prefix='/api')

# Constants
SEARCH_URL = "https://new4.scloud.ninja/get-search-token"
SEARCH_RESULTS_URL = "https://new4.scloud.ninja/?token="
USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.3 Safari/605.1.15"

# Set session for reuse
session = requests.Session()

@api_bp.route('/search', methods=['GET'])
def search():
    """Search for files on scloud.ninja."""
    try:
        # Get search query from request
        search_query = request.args.get('search', '')
        if not search_query:
            return jsonify({"error": "Search query is required"}), 400
        
        logger.info(f"Searching for: {search_query}")
        
        # Step 1: Get Search Token
        headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Content-Type': 'application/x-www-form-urlencoded',
            'Origin': 'https://new4.scloud.ninja',
            'Referer': 'https://new4.scloud.ninja/',
            'User-Agent': USER_AGENT
        }
        
        # Prepare form data
        form_data = f"search_query={urllib.parse.quote(search_query)}"
        
        # Make POST request to get token
        token_response = session.post(
            SEARCH_URL,
            data=form_data,
            headers=headers,
            allow_redirects=False
        )
        
        # Check if response contains redirect with token
        if token_response.status_code != 302:
            logger.error(f"Invalid response from token request: {token_response.status_code}")
            return jsonify({"error": "Failed to get search token"}), 500
        
        # Extract token from Location header
        location_header = token_response.headers.get('location', '')
        token_match = re.search(r'token=([a-f0-9\-]+)', location_header)
        
        if not token_match:
            logger.error("Failed to extract token from redirect URL")
            return jsonify({"error": "Failed to extract search token"}), 500
        
        token = token_match.group(1)
        logger.debug(f"Extracted token: {token}")
        
        # Step 2: Fetch search results with token
        results_url = f"{SEARCH_RESULTS_URL}{token}"
        results_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'User-Agent': USER_AGENT,
            'Referer': 'https://new4.scloud.ninja/'
        }
        
        results_response = session.get(results_url, headers=results_headers)
        
        if results_response.status_code != 200:
            logger.error(f"Failed to fetch search results: {results_response.status_code}")
            return jsonify({"error": "Failed to fetch search results"}), 500
        
        # Parse HTML response
        results = parse_search_results(results_response.text)
        
        return jsonify(results)
    
    except Exception as e:
        logger.exception(f"Search error: {str(e)}")
        return jsonify({"error": f"Search failed: {str(e)}"}), 500

@api_bp.route('/download', methods=['GET'])
def download():
    """Get download link for a specific file."""
    try:
        # Get file ID from request
        file_id = request.args.get('id', '')
        if not file_id:
            return jsonify({"error": "File ID is required"}), 400
            
        logger.info(f"Getting download link for file ID: {file_id}")
        
        # Construct file URL
        file_url = f"https://new3.scloud.ninja/f/{file_id}"
        
        # Send request to get file page
        headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'User-Agent': USER_AGENT
        }
        
        response = session.get(file_url, headers=headers)
        
        if response.status_code != 200:
            logger.error(f"Failed to fetch file details: {response.status_code}")
            return jsonify({"error": "Failed to fetch file details"}), 500
        
        # Parse HTML to extract download link and file details
        soup = BeautifulSoup(response.text, 'html.parser')
        
        try:
            # The website structure has changed - let's update our selectors
            
            # For debugging
            logger.debug(f"Response content length: {len(response.text)}")
            
            # Handle the case where the file details page has updated structure
            # Extract download link - look for links with download attributes or matching patterns
            download_links = soup.select('a[href^="https://"]')
            download_link = None
            for link in download_links:
                href = link.get('href')
                if href and ('download' in href.lower() or '/dl/' in href):
                    download_link = href
                    break
                    
            # Enhanced filename extraction - try different selectors
            filename = file_id  # Default to file ID if we can't find the name
            
            # Try various selectors that might contain the filename
            filename_candidates = [
                soup.select_one('.title-container span'),
                soup.select_one('h1'),
                soup.select_one('h2'),
                soup.select_one('title'),
                soup.select_one('strong')
            ]
            
            for candidate in filename_candidates:
                if candidate and candidate.text.strip():
                    filename = candidate.text.strip()
                    if 'scloud' not in filename.lower():  # Skip if it's just the site name
                        break
                        
            # Try to find file size
            size = "Unknown size"
            size_candidates = [
                soup.select_one('span.inline-block'),
                soup.select_one('.file-size'),
                soup.select_one('.size')
            ]
            
            for candidate in size_candidates:
                if candidate and candidate.text.strip():
                    size = candidate.text.strip()
                    break
            
            # As a backup approach, look for text containing common file size patterns
            if size == "Unknown size":
                size_pattern = re.compile(r'(\d+(\.\d+)?\s*(KB|MB|GB|TB))', re.IGNORECASE)
                text_elements = soup.find_all(text=True)
                
                for text in text_elements:
                    match = size_pattern.search(text)
                    if match:
                        size = match.group(0)
                        break
                
            # Return results with the original file ID as fallback filename
            result = {
                "filename": filename or "Unknown",
                "size": size,
                "downloadLink": download_link or f"https://new3.scloud.ninja/dl/{file_id}"
            }
            
            # Since we're generating a fallback download link, note this
            if not download_link:
                logger.warning(f"Generated fallback download link for file ID: {file_id}")
            
            # Debug log for troubleshooting
            logger.debug(f"File details extracted: {result}")
        except Exception as e:
            logger.exception(f"Error parsing file details: {str(e)}")
            
            # Create a fallback result if parsing fails - use original file ID as a fallback
            result = {
                "filename": file_id,  # Use file ID as fallback
                "size": "File size unavailable",
                "downloadLink": f"https://new3.scloud.ninja/dl/{file_id}"  # Construct a direct download link
            }
        
        return jsonify(result)
        
    except Exception as e:
        logger.exception(f"Download error: {str(e)}")
        return jsonify({"error": f"Failed to get download link: {str(e)}"}), 500

def parse_search_results(html_content):
    """Parse the search results HTML and extract file information."""
    try:
        result = []
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Find all file links
        file_links = soup.select('a[href]')
        
        for link in file_links:
            title_elem = link.select_one('.title-container span')
            size_elem = link.select_one('span.inline-block')
            href = link.get('href')
            
            # Skip if any required data is missing
            if not (title_elem and size_elem and href):
                continue
            
            title = title_elem.text.strip()
            size = size_elem.text.strip()
            
            # Extract file ID from link
            file_id = href.split('/')[-1] if href and isinstance(href, str) else ""
            
            if title and size and file_id:
                result.append({
                    "name": title,
                    "size": size,
                    "link": file_id
                })
        
        return result
    except Exception as e:
        logger.exception(f"Error parsing search results: {str(e)}")
        return []